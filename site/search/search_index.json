{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Uberon Ontology Documentation Quick links: Uberon on Github Issue Tracker Browse Uberon on OLS","title":"Getting started"},{"location":"#uberon-ontology-documentation","text":"","title":"Uberon Ontology Documentation"},{"location":"#quick-links","text":"Uberon on Github Issue Tracker Browse Uberon on OLS","title":"Quick links:"},{"location":"about/","text":"About UBERON Uberon is an anatomical ontology that represents body parts, organs and tissues in a variety of animal species, with a focus on vertebrates. It has been constructed to integrate seamlessly with other ontologies, such as the OBO Cell Ontology workflows , the Gene Ontology , Trait and Phenotype ontologies , as well as other anatomical ontologies. The ontology includes comprehensive relationships to taxon-specific anatomical ontologies, allowing integration of functional, phenotype and expression data. The figure below shows taxon-centric anatomy ontologies along the bottom axis; domain specific ontologies on the right hand side; orthogonal ontologies on the left axis. Currently Uberon consists of over 13000 classes representing structures that are shared across a variety of metazoans. As one of the main uses of Uberon is translational science, we have extensive coverage of structures shared between humans and other species. However, thanks to the involvement of many collaborators, we have deep coverage of broad areas of anatomy across diverse taxa. We also make available an ontology called composite-metazoan which brings in subsets of federated ontologies, with a total of over 40000 classes.","title":"About Uberon"},{"location":"about/#about-uberon","text":"Uberon is an anatomical ontology that represents body parts, organs and tissues in a variety of animal species, with a focus on vertebrates. It has been constructed to integrate seamlessly with other ontologies, such as the OBO Cell Ontology workflows , the Gene Ontology , Trait and Phenotype ontologies , as well as other anatomical ontologies. The ontology includes comprehensive relationships to taxon-specific anatomical ontologies, allowing integration of functional, phenotype and expression data. The figure below shows taxon-centric anatomy ontologies along the bottom axis; domain specific ontologies on the right hand side; orthogonal ontologies on the left axis. Currently Uberon consists of over 13000 classes representing structures that are shared across a variety of metazoans. As one of the main uses of Uberon is translational science, we have extensive coverage of structures shared between humans and other species. However, thanks to the involvement of many collaborators, we have deep coverage of broad areas of anatomy across diverse taxa. We also make available an ontology called composite-metazoan which brings in subsets of federated ontologies, with a total of over 40000 classes.","title":"About UBERON"},{"location":"acknowledgements/","text":"Contributing Organizations See the list of adopters , many of whom are also contributing content to the ontology. Ontologies Uberon has been developed in conjunction with a number of other bio-ontologies, in particular - ZFA (Zebrafish) - XAO (Xenopus) - TAO (Teleost, now included in Uberon) - AAO (Amphibia, now included in Uberon) - VSAO (Vertebrate Skeleton, now included in Uberon) - MA (Adult Mouse) - EMAPA (Developmental Mouse) - EHDAA2 (Developmental Human) - FMA (Adult Human) - FBbt (Drosophila) - WBbt (C elegans) - MP (Mouse Phenotype) - HP (Human Phenotype) - GO (Gene Ontology) - FEED (Mammalian Feeding Muscles) - CL (Cell Type) Technology Uberon shares a similar technology stack to many other bio-ontologies. Key contributors to this stack include Heiko Dietze, Seth Carbon, James Balhoff, Frederic Bastian, Alan Ruttenberg, David Osumi-Sutherland. The Elk Reasoner We use a variety of reasoners, but like most bio-ontologies, the game changer for us has been the fantastic Elk reasoner . ELK has been created in the Knowledge Representation and Reasoning group at the Department of Computer Science of the University of Oxford. Development has been supported by the EPSRC under the research project ConDOR: Consequence-Driven Ontology Reasoning (grant number EP/G02085X/1). We are also grateful to the developers of HermiT and FACT++, which are also used during development. OWL API The OWL API is a Java API and reference implmentation for creating, manipulating and serialising OWL Ontologies. The current Uberon development and build infrastructure relies heavily on the OWLAPI. Many thanks to the OWLAPI developers, especially Ignazio Palmisano and Matt Horridge. In particular, we make use of an in-house library developed for the GO called owltools . SWI-Prolog Early versions of the ontology were created using a combination of text mining and rule-based reasoning approaches. This would not have been possible without SWI-Prolog . Hosting We use github for hosting.","title":"Acknowledgements"},{"location":"acknowledgements/#contributing-organizations","text":"See the list of adopters , many of whom are also contributing content to the ontology.","title":"Contributing Organizations"},{"location":"acknowledgements/#ontologies","text":"Uberon has been developed in conjunction with a number of other bio-ontologies, in particular - ZFA (Zebrafish) - XAO (Xenopus) - TAO (Teleost, now included in Uberon) - AAO (Amphibia, now included in Uberon) - VSAO (Vertebrate Skeleton, now included in Uberon) - MA (Adult Mouse) - EMAPA (Developmental Mouse) - EHDAA2 (Developmental Human) - FMA (Adult Human) - FBbt (Drosophila) - WBbt (C elegans) - MP (Mouse Phenotype) - HP (Human Phenotype) - GO (Gene Ontology) - FEED (Mammalian Feeding Muscles) - CL (Cell Type)","title":"Ontologies"},{"location":"acknowledgements/#technology","text":"Uberon shares a similar technology stack to many other bio-ontologies. Key contributors to this stack include Heiko Dietze, Seth Carbon, James Balhoff, Frederic Bastian, Alan Ruttenberg, David Osumi-Sutherland.","title":"Technology"},{"location":"acknowledgements/#the-elk-reasoner","text":"We use a variety of reasoners, but like most bio-ontologies, the game changer for us has been the fantastic Elk reasoner . ELK has been created in the Knowledge Representation and Reasoning group at the Department of Computer Science of the University of Oxford. Development has been supported by the EPSRC under the research project ConDOR: Consequence-Driven Ontology Reasoning (grant number EP/G02085X/1). We are also grateful to the developers of HermiT and FACT++, which are also used during development.","title":"The Elk Reasoner"},{"location":"acknowledgements/#owl-api","text":"The OWL API is a Java API and reference implmentation for creating, manipulating and serialising OWL Ontologies. The current Uberon development and build infrastructure relies heavily on the OWLAPI. Many thanks to the OWLAPI developers, especially Ignazio Palmisano and Matt Horridge. In particular, we make use of an in-house library developed for the GO called owltools .","title":"OWL API"},{"location":"acknowledgements/#swi-prolog","text":"Early versions of the ontology were created using a combination of text mining and rule-based reasoning approaches. This would not have been possible without SWI-Prolog .","title":"SWI-Prolog"},{"location":"acknowledgements/#hosting","text":"We use github for hosting.","title":"Hosting"},{"location":"adopters/","text":"Adopters Phenoscape The Phenoscape projects collects and analyzes phenotypic character descriptions from a range of vertebrate species, using Uberon and PATO to describe evolutionary character states. The Phenoscape project is both a major driver of and contributor to Uberon, contibuting thousands of terms. The teleost (bony fishes) component of Uberon was derived from the Teleost Anatomy Ontology, developed by the Phenoscape group. Most of the high level design of the skeletal system comes from the Vertebrate Skeletal Anatomy Ontology (VSAO), also created by the Phenoscape group. Phenoscape curators continue to extend the ontology, covering a wide variety of tetrapod structures, with an emphasis on the appendicular system. See the Phenoscape website for more details Bgee Bgee is a database to retrieve and compare gene expression patterns between animal species. Bgee in using Uberon to annotate the site of expression, and Bgee curators one the major contributors to the ontology. BGee Gene Ontology The Gene Ontology uses Uberon to classify developmental processes, and to provide additional contextual information on annotations, such as the location of a biological process. See the GO website Monarch Initiative Model systems are the cornerstone of biomedical research to investigate biological processes, test gene-based disease hypotheses, and develop and test disease treatments. The vast knowledge that we have about model systems can be better utilized if semantically aggregated and made queryable based on any number of facets, such as phenotypic similarity, network analysis, gene expression and function, and genomics. The Monarch Initiative aims to provide easy-to-use tools to navigate this data landscape, services for other resources, and educational outreach regarding the production of structured data for biomedical discovery. The user of Uberon to bridge between different species is key to the phenotype mapping component of this project. See the Monarch Initiative website for more details EBI The EBI Samples Phenotypes and Ontology Team is using Uberon for describing biological samples and phenotypes. Recently, Uberon was integrated with the EFO (Experimental Factory Ontology), developed by the SPOT group. Global Alliance for Genomes and Health The Global Alliance for Genomes and Health is an international coalition, dedicated to improving human health by maximizing the potential of genomic medicine through effective and responsible data sharing. The Global Alliance Data Working Group has proposed Uberon and the CL as a standard for the description of biological samples. See the metadata schema: eagle i The eagle i project uses Uberon anatomical structures to collect information about cell lines and biospecimens. Since eagle-i is often concerned with non-model organism resources, Uberon + Taxonomy is the ideal mechanism to assert the source anatomical structure for these resources. FANTOM5 FANTOM is an international research consortium established by Dr. Hayashizaki and his colleagues in 2000 to assign functional annotations to the full-length cDNAs that were collected during the Mouse Encyclopedia Project at RIKEN. FANTOM has since developed and expanded over time to encompass the fields of transcriptome analysis. The object of the project is moving steadily up the layers in the system of life, progressing thus from an understanding of the \u2018elements\u2019 - the transcripts - to an understanding of the \u2018system\u2019 - the transcriptional regulatory network, in other words the \u2018system\u2019 of an individual life form. FANTOM5 is using Uberon and CL to annotate samples allowing for transcriptome analyses with cell-type and tissue-level specificity. See FANTOM website ENCODE The National Human Genome Research Institute (NHGRI) launched a public research consortium named ENCODE, the Encyclopedia Of DNA Elements, in September 2003, to carry out a project to identify all functional elements in the human genome sequence. The ENCODE Data Collection Center (DCC) uses a core set of ontologies: - UBERON - CL - EFO - OBI - ChEBI - SO - GO - See ENCODE DCC Ontologies Malladi, V. S., Erickson, D. T., Podduturi, N. R., Rowe, L. D., Chan, E. T., Davidson, J. M., \u2026 Hong, E. L. (2015). Ontology application and use at the ENCODE DCC. Database : The Journal of Biological Databases and Curation, 2015, bav010\u2013. doi:10.1093/database/bav010 Sloan, C. A., Chan, E. T., Davidson, J. M., Malladi, V. S., Strattan, J. S., Hitz, B. C., \u2026 Cherry, J. M. (2015). ENCODE data at the ENCODE portal. Nucleic Acids Research, gkv1160\u2013. doi:10.1093/nar/gkv1160 neXtProt neXtProt is an on-line knowledge platform on human proteins. It strives to be a comprehensive resource that provides a variety of types of information on human proteins, such as their function, subcellular location, expression, interactions and role in diseases. neXtProt is using Uberon as the main vaculary for describing site of protein localization in animals. FEED The NSF FEED (Feeding Experiments End-user Group) working group is developing a database of mammalian feeding muscle data. The FEED developers are responsible for the craniofacial muscle aspects of Uberon. SciCrunch Uberon is the anatomical ontology used as part of the SciCrunch integrated search system and dkNET projects. CELLPEDIA See Hatano et al in Database International Human Epigenomics Consortium The International Human Epigenome Consortium (IHEC) is a global consortium with the primary goal of providing free access to high-resolution reference human epigenome maps for normal and disease cell types to the research community. IHEC is working to define standards for epigenomic mapping and metadata. Uberon and CL are the IHEC standard ontologies for tissue and cell line names. For more details, see the IHEC Standards page . NIH LINCS The NIH Library of Integrated Network-based Cellular Signatures (LINCS) project aims to create a network-based understanding of biology by cataloging changes in gene expression and other cellular processes that occur when cells are exposed to a variety of perturbing agents, and by using computational tools to integrate this diverse information into a comprehensive view of normal and disease states that can be applied for the development of new biomarkers and therapeutics. This project recommends the usage of Uberon and the OBO Cell Ontology for describing organs, tissues and cells for annotating reagents and assays. See Vempati et al The Alexandria Archive Institute In September, 2012, the AAI launched Exploring Biogeography of Early Domestic Animals using Linked Open Data , a one-year project using Linked Open Data to enhance archaeological data sets. Skeletal element data are also linked using Uberon. See: linked data Mixing Models for Communicating Research Data in Archaeology, Kansa et al, International Journal for Digital Curation,Vol 9, No. 1 (2014)","title":"Adopters"},{"location":"adopters/#adopters","text":"","title":"Adopters"},{"location":"adopters/#phenoscape","text":"The Phenoscape projects collects and analyzes phenotypic character descriptions from a range of vertebrate species, using Uberon and PATO to describe evolutionary character states. The Phenoscape project is both a major driver of and contributor to Uberon, contibuting thousands of terms. The teleost (bony fishes) component of Uberon was derived from the Teleost Anatomy Ontology, developed by the Phenoscape group. Most of the high level design of the skeletal system comes from the Vertebrate Skeletal Anatomy Ontology (VSAO), also created by the Phenoscape group. Phenoscape curators continue to extend the ontology, covering a wide variety of tetrapod structures, with an emphasis on the appendicular system. See the Phenoscape website for more details","title":"Phenoscape"},{"location":"adopters/#bgee","text":"Bgee is a database to retrieve and compare gene expression patterns between animal species. Bgee in using Uberon to annotate the site of expression, and Bgee curators one the major contributors to the ontology. BGee","title":"Bgee"},{"location":"adopters/#gene-ontology","text":"The Gene Ontology uses Uberon to classify developmental processes, and to provide additional contextual information on annotations, such as the location of a biological process. See the GO website","title":"Gene Ontology"},{"location":"adopters/#monarch-initiative","text":"Model systems are the cornerstone of biomedical research to investigate biological processes, test gene-based disease hypotheses, and develop and test disease treatments. The vast knowledge that we have about model systems can be better utilized if semantically aggregated and made queryable based on any number of facets, such as phenotypic similarity, network analysis, gene expression and function, and genomics. The Monarch Initiative aims to provide easy-to-use tools to navigate this data landscape, services for other resources, and educational outreach regarding the production of structured data for biomedical discovery. The user of Uberon to bridge between different species is key to the phenotype mapping component of this project. See the Monarch Initiative website for more details","title":"Monarch Initiative"},{"location":"adopters/#ebi","text":"The EBI Samples Phenotypes and Ontology Team is using Uberon for describing biological samples and phenotypes. Recently, Uberon was integrated with the EFO (Experimental Factory Ontology), developed by the SPOT group.","title":"EBI"},{"location":"adopters/#global-alliance-for-genomes-and-health","text":"The Global Alliance for Genomes and Health is an international coalition, dedicated to improving human health by maximizing the potential of genomic medicine through effective and responsible data sharing. The Global Alliance Data Working Group has proposed Uberon and the CL as a standard for the description of biological samples. See the metadata schema:","title":"Global Alliance for Genomes and Health"},{"location":"adopters/#eagle-i","text":"The eagle i project uses Uberon anatomical structures to collect information about cell lines and biospecimens. Since eagle-i is often concerned with non-model organism resources, Uberon + Taxonomy is the ideal mechanism to assert the source anatomical structure for these resources.","title":"eagle i"},{"location":"adopters/#fantom5","text":"FANTOM is an international research consortium established by Dr. Hayashizaki and his colleagues in 2000 to assign functional annotations to the full-length cDNAs that were collected during the Mouse Encyclopedia Project at RIKEN. FANTOM has since developed and expanded over time to encompass the fields of transcriptome analysis. The object of the project is moving steadily up the layers in the system of life, progressing thus from an understanding of the \u2018elements\u2019 - the transcripts - to an understanding of the \u2018system\u2019 - the transcriptional regulatory network, in other words the \u2018system\u2019 of an individual life form. FANTOM5 is using Uberon and CL to annotate samples allowing for transcriptome analyses with cell-type and tissue-level specificity. See FANTOM website","title":"FANTOM5"},{"location":"adopters/#encode","text":"The National Human Genome Research Institute (NHGRI) launched a public research consortium named ENCODE, the Encyclopedia Of DNA Elements, in September 2003, to carry out a project to identify all functional elements in the human genome sequence. The ENCODE Data Collection Center (DCC) uses a core set of ontologies: - UBERON - CL - EFO - OBI - ChEBI - SO - GO - See ENCODE DCC Ontologies Malladi, V. S., Erickson, D. T., Podduturi, N. R., Rowe, L. D., Chan, E. T., Davidson, J. M., \u2026 Hong, E. L. (2015). Ontology application and use at the ENCODE DCC. Database : The Journal of Biological Databases and Curation, 2015, bav010\u2013. doi:10.1093/database/bav010 Sloan, C. A., Chan, E. T., Davidson, J. M., Malladi, V. S., Strattan, J. S., Hitz, B. C., \u2026 Cherry, J. M. (2015). ENCODE data at the ENCODE portal. Nucleic Acids Research, gkv1160\u2013. doi:10.1093/nar/gkv1160","title":"ENCODE"},{"location":"adopters/#nextprot","text":"neXtProt is an on-line knowledge platform on human proteins. It strives to be a comprehensive resource that provides a variety of types of information on human proteins, such as their function, subcellular location, expression, interactions and role in diseases. neXtProt is using Uberon as the main vaculary for describing site of protein localization in animals.","title":"neXtProt"},{"location":"adopters/#feed","text":"The NSF FEED (Feeding Experiments End-user Group) working group is developing a database of mammalian feeding muscle data. The FEED developers are responsible for the craniofacial muscle aspects of Uberon.","title":"FEED"},{"location":"adopters/#scicrunch","text":"Uberon is the anatomical ontology used as part of the SciCrunch integrated search system and dkNET projects.","title":"SciCrunch"},{"location":"adopters/#cellpedia","text":"See Hatano et al in Database","title":"CELLPEDIA"},{"location":"adopters/#international-human-epigenomics-consortium","text":"The International Human Epigenome Consortium (IHEC) is a global consortium with the primary goal of providing free access to high-resolution reference human epigenome maps for normal and disease cell types to the research community. IHEC is working to define standards for epigenomic mapping and metadata. Uberon and CL are the IHEC standard ontologies for tissue and cell line names. For more details, see the IHEC Standards page .","title":"International Human Epigenomics Consortium"},{"location":"adopters/#nih-lincs","text":"The NIH Library of Integrated Network-based Cellular Signatures (LINCS) project aims to create a network-based understanding of biology by cataloging changes in gene expression and other cellular processes that occur when cells are exposed to a variety of perturbing agents, and by using computational tools to integrate this diverse information into a comprehensive view of normal and disease states that can be applied for the development of new biomarkers and therapeutics. This project recommends the usage of Uberon and the OBO Cell Ontology for describing organs, tissues and cells for annotating reagents and assays. See Vempati et al","title":"NIH LINCS"},{"location":"adopters/#the-alexandria-archive-institute","text":"In September, 2012, the AAI launched Exploring Biogeography of Early Domestic Animals using Linked Open Data , a one-year project using Linked Open Data to enhance archaeological data sets. Skeletal element data are also linked using Uberon. See: linked data Mixing Models for Communicating Research Data in Archaeology, Kansa et al, International Journal for Digital Curation,Vol 9, No. 1 (2014)","title":"The Alexandria Archive Institute"},{"location":"bridges/","text":"Uberon bridge files The latest bridge files can be found on GitHub .","title":"Cross-species bridge files"},{"location":"bridges/#uberon-bridge-files","text":"The latest bridge files can be found on GitHub .","title":"Uberon bridge files"},{"location":"cite/","text":"How to cite UBERON Uberon, an integrative multi-species anatomy ontology Mungall, C. J., Torniai, C., Gkoutos, G. V., Lewis, S. E., and Haendel, M. A. (2012) Genome Biology 13, R5. PMID:22293552 Unification of multi-species vertebrate anatomy ontologies for comparative biology in Uberon Haendel, Melissa A; Balhoff, James P; Bastian, Frederic B; Blackburn, David C; Blake, Judith A; Bradford, Yvonne; Comte, Aurelie; Dahdul, Wasila M; Dececchi, Thomas A; Druzinsky, Robert E; Hayamizu, Terry F; Ibrahim, Nizar; Lewis, Suzanna E; Mabee, Paula M; Niknejad, Anne; Robinson-Rechavi, Marc; Sereno, Paul C; Mungall, Christopher J (2014) Journal of Biomedical Semantics, 5(1), 21. doi:10.1186/2041-1480-5-21 Papers referencing UBERON Google scholar link Posters and Presentatations Structuring Phenotype Data for Invertebrate Genomes - Chris Mungall, GIGA2 2015 From baleen to cleft palate: an ontological exploration of evolution and disease - Melissa Haendel, Keynote, Bio-Ontologies 2014 Semantics of and for the diversity of life:\u2028 Opportunities and perils of trying to reason on the frontier - Hilmar Lapp, Keynote, CSHALS 2014 Uberon - Chris Mungall, EBI Industry Workshop, 2013 Use of Uberon in the Bgee database - Frederic Bastian, Biocuration, 2013 Using ontologies to enhance integration and analyses of ENCODE data - Venkat Malladi, Biocuration, 2013 Uberon and (Zoo)archaeology - Eric Kansa, NSF Phenotype RCN 2012 Hiding ontologies under the carpet - Frederic Bastian, OntoSIB, 2012 Uberon - Melissa Haendel, Cell Ontology Workshop, 2010","title":"Cite"},{"location":"cite/#how-to-cite-uberon","text":"Uberon, an integrative multi-species anatomy ontology Mungall, C. J., Torniai, C., Gkoutos, G. V., Lewis, S. E., and Haendel, M. A. (2012) Genome Biology 13, R5. PMID:22293552 Unification of multi-species vertebrate anatomy ontologies for comparative biology in Uberon Haendel, Melissa A; Balhoff, James P; Bastian, Frederic B; Blackburn, David C; Blake, Judith A; Bradford, Yvonne; Comte, Aurelie; Dahdul, Wasila M; Dececchi, Thomas A; Druzinsky, Robert E; Hayamizu, Terry F; Ibrahim, Nizar; Lewis, Suzanna E; Mabee, Paula M; Niknejad, Anne; Robinson-Rechavi, Marc; Sereno, Paul C; Mungall, Christopher J (2014) Journal of Biomedical Semantics, 5(1), 21. doi:10.1186/2041-1480-5-21","title":"How to cite UBERON"},{"location":"cite/#papers-referencing-uberon","text":"Google scholar link","title":"Papers referencing UBERON"},{"location":"cite/#posters-and-presentatations","text":"Structuring Phenotype Data for Invertebrate Genomes - Chris Mungall, GIGA2 2015 From baleen to cleft palate: an ontological exploration of evolution and disease - Melissa Haendel, Keynote, Bio-Ontologies 2014 Semantics of and for the diversity of life:\u2028 Opportunities and perils of trying to reason on the frontier - Hilmar Lapp, Keynote, CSHALS 2014 Uberon - Chris Mungall, EBI Industry Workshop, 2013 Use of Uberon in the Bgee database - Frederic Bastian, Biocuration, 2013 Using ontologies to enhance integration and analyses of ENCODE data - Venkat Malladi, Biocuration, 2013 Uberon and (Zoo)archaeology - Eric Kansa, NSF Phenotype RCN 2012 Hiding ontologies under the carpet - Frederic Bastian, OntoSIB, 2012 Uberon - Melissa Haendel, Cell Ontology Workshop, 2010","title":"Posters and Presentatations"},{"location":"combined_multispecies/","text":"Combined Multispecies Ontologies Composite (merged) Multispecies Ontologies See the wiki for more details. Composite ontologies merge species ontologies into the structure of Uberon, merging in taxonomic equivalents, and relabeling species-specific classes. Note the composite ontologies also merge in all of the Cell Ontology (CL). | name | obo | owl | includes | | composite-vertebrate | .obo | .owl | MA, EHDAA2, ZFA, XAO, CL | | composite-metazoan | .obo | .owl | MA, EHDAA2, ZFA, XAO, CL, FBbt, WBbt, PORO, CTENO, CEPH | | composite-metazoan-basic | .obo | n/a | MA, EHDAA2, ZFA, XAO, CL, FBbt, WBbt, PORO, CTENO, CEPH | Importer (collector) Multispecies Ontologies See the wiki for more details. See also Fig 3 from the Uberon paper: The ontology collected-eukaryote.owl imports Uberon plus bridging axioms plus individual species anatomy ontologies. The ontology is constructed recursively, and it is possible to obtain an importer ontology for a variety of taxa.","title":"Combined Multispecies Ontologies"},{"location":"combined_multispecies/#combined-multispecies-ontologies","text":"","title":"Combined Multispecies Ontologies"},{"location":"combined_multispecies/#composite-merged-multispecies-ontologies","text":"See the wiki for more details. Composite ontologies merge species ontologies into the structure of Uberon, merging in taxonomic equivalents, and relabeling species-specific classes. Note the composite ontologies also merge in all of the Cell Ontology (CL). | name | obo | owl | includes | | composite-vertebrate | .obo | .owl | MA, EHDAA2, ZFA, XAO, CL | | composite-metazoan | .obo | .owl | MA, EHDAA2, ZFA, XAO, CL, FBbt, WBbt, PORO, CTENO, CEPH | | composite-metazoan-basic | .obo | n/a | MA, EHDAA2, ZFA, XAO, CL, FBbt, WBbt, PORO, CTENO, CEPH |","title":"Composite (merged) Multispecies Ontologies"},{"location":"combined_multispecies/#importer-collector-multispecies-ontologies","text":"See the wiki for more details. See also Fig 3 from the Uberon paper: The ontology collected-eukaryote.owl imports Uberon plus bridging axioms plus individual species anatomy ontologies. The ontology is constructed recursively, and it is possible to obtain an importer ontology for a variety of taxa.","title":"Importer (collector) Multispecies Ontologies"},{"location":"contributing/","text":"How to contribute to UBERON GitHub Tracker Our preferred way of receiving requests for new terms, changes to the ontology or questions is by creating a new issue using our issue tracker: Current list of issues Submit a new issue Mailing List The obo-anatomy mail list is a community listserve for all anatomical ontologies: - Subscribe via google groups Follow us Twitter","title":"Contributing"},{"location":"contributing/#how-to-contribute-to-uberon","text":"","title":"How to contribute to UBERON"},{"location":"contributing/#github-tracker","text":"Our preferred way of receiving requests for new terms, changes to the ontology or questions is by creating a new issue using our issue tracker: Current list of issues Submit a new issue","title":"GitHub Tracker"},{"location":"contributing/#mailing-list","text":"The obo-anatomy mail list is a community listserve for all anatomical ontologies: - Subscribe via google groups","title":"Mailing List"},{"location":"contributing/#follow-us","text":"Twitter","title":"Follow us"},{"location":"current_release/","text":"","title":"Current release"},{"location":"old_releases/","text":"Releases prior to migration to GitHub The releases prior to October 2020 can be found here .","title":"Releases before 2020"},{"location":"old_releases/#releases-prior-to-migration-to-github","text":"The releases prior to October 2020 can be found here .","title":"Releases prior to migration to GitHub"},{"location":"uberon_build_pipeline/","text":"The Uberon Build System Overview of Migration to ODK (Q2 2021) Problem : The Uberon pipeline is the single most complex overall pipeline for any ontology. It has grown so complex over the years that only one person could run it, let alone understand it: Chris. Goal : The goal is to migrate the Uberon pipeline to a more standard ODK-like setup. A lot of what is happening in the pipeline is out of scope for ODK (taxon constraints, cross-species bridges), but other things can be standardised (ROBOT report, DOSDP pattern workflows, imports). Ultimately, we want four people at least to be able to run releases, to reduce the \"Bus factor\" of the project. We also want to generally share our understanding of the pipeline and document it better. To achieve this, the following rough steps are performed. 1. An ODK config ( src/ontology/uberon-odk.yaml ) file is created that defines all the relevant release artefacts for Uberon 1. The repository structure is moved to the typical ODK layout, which means - the edit file is now in src/ontology/uberon-edit.obo - the custom Make pipeline now moved to src/ontology/uberon.Makefile - this is where the magic happens. 1. The pipeline is run with sh run.sh make all -B 1. We are reviewing the a number of key release artefacts and try to get them to an OK state. Build recipe of key goals Intermediate artefacts (other than imports and mirrors): composite-metazoan.owl (super weird: odk version has imports while last release version does not) tmp/unreasoned-composite-metazoan.owl ext-weak.owl (ext without disjoint classes) ext.owl (does not trigger imports along the way!) tmp/materialized.owl tmp/unreasoned.owl tmp/uberon-edit.owl uberon-edit.obo components/disjoint_union_over.ofn components/phenoscape-ext.owl bridge/uberon-bridge-to-bfo.owl components/reflexivity_axioms.owl Many allen-% and local-% imports For some reason the bridges are also in the dependencies (seems FMA bridge is needed) uberon.owl ext.owl tmp/materialized.owl tmp/unreasoned.owl tmp/uberon-edit.owl uberon-edit.obo components/disjoint_union_over.ofn components/phenoscape-ext.owl bridge/uberon-bridge-to-bfo.owl components/reflexivity_axioms.owl subsets/human-view.owl: ext.owl (...) contexts/context-human.owl subsets/euarchontoglires-basic.owl taxmods/uberon-taxmod-euarchontoglires.owl tmp/uberon-taxmod-314146.owl ext.owl (...) core.owl (does not seem to be a real dependency) tmp/uberon-edit.owl uberon-edit.obo components/disjoint_union_over.ofn Other dependecies: prepare_patterns, update_patterns, pattern_schema_checks ../patterns/definitions.owl QC checks: reports/uberon-edit.obo-gocheck (tmp/GO.xrf_abbs) reports/uberon-edit.obo-iconv .... Recurring stuff: (ignore this) Download mirrors (some preprocessing involved) Examples: tmp/fixed-emapa.obo tmp/mirror-emapa.obo tmp/mirror-zfa.obo tmp/fixed-zfa.obo imports/ncbitaxon_import.owl tmp/composite-stages.obo tmp/merged-stages-xrefs.obo tmp/update-stages tmp/developmental-stage-ontologies/src/ssso-merged.obo tmp/developmental-stage-ontologies/src/mmusdv/mmusdv.obo Imports tmp/seed.owl tmp/cl-core.obo","title":"Uberon Build Pipeline"},{"location":"uberon_build_pipeline/#the-uberon-build-system","text":"","title":"The Uberon Build System"},{"location":"uberon_build_pipeline/#overview-of-migration-to-odk-q2-2021","text":"Problem : The Uberon pipeline is the single most complex overall pipeline for any ontology. It has grown so complex over the years that only one person could run it, let alone understand it: Chris. Goal : The goal is to migrate the Uberon pipeline to a more standard ODK-like setup. A lot of what is happening in the pipeline is out of scope for ODK (taxon constraints, cross-species bridges), but other things can be standardised (ROBOT report, DOSDP pattern workflows, imports). Ultimately, we want four people at least to be able to run releases, to reduce the \"Bus factor\" of the project. We also want to generally share our understanding of the pipeline and document it better. To achieve this, the following rough steps are performed. 1. An ODK config ( src/ontology/uberon-odk.yaml ) file is created that defines all the relevant release artefacts for Uberon 1. The repository structure is moved to the typical ODK layout, which means - the edit file is now in src/ontology/uberon-edit.obo - the custom Make pipeline now moved to src/ontology/uberon.Makefile - this is where the magic happens. 1. The pipeline is run with sh run.sh make all -B 1. We are reviewing the a number of key release artefacts and try to get them to an OK state.","title":"Overview of Migration to ODK (Q2 2021)"},{"location":"uberon_build_pipeline/#build-recipe-of-key-goals","text":"Intermediate artefacts (other than imports and mirrors): composite-metazoan.owl (super weird: odk version has imports while last release version does not) tmp/unreasoned-composite-metazoan.owl ext-weak.owl (ext without disjoint classes) ext.owl (does not trigger imports along the way!) tmp/materialized.owl tmp/unreasoned.owl tmp/uberon-edit.owl uberon-edit.obo components/disjoint_union_over.ofn components/phenoscape-ext.owl bridge/uberon-bridge-to-bfo.owl components/reflexivity_axioms.owl Many allen-% and local-% imports For some reason the bridges are also in the dependencies (seems FMA bridge is needed) uberon.owl ext.owl tmp/materialized.owl tmp/unreasoned.owl tmp/uberon-edit.owl uberon-edit.obo components/disjoint_union_over.ofn components/phenoscape-ext.owl bridge/uberon-bridge-to-bfo.owl components/reflexivity_axioms.owl subsets/human-view.owl: ext.owl (...) contexts/context-human.owl subsets/euarchontoglires-basic.owl taxmods/uberon-taxmod-euarchontoglires.owl tmp/uberon-taxmod-314146.owl ext.owl (...) core.owl (does not seem to be a real dependency) tmp/uberon-edit.owl uberon-edit.obo components/disjoint_union_over.ofn Other dependecies: prepare_patterns, update_patterns, pattern_schema_checks ../patterns/definitions.owl QC checks: reports/uberon-edit.obo-gocheck (tmp/GO.xrf_abbs) reports/uberon-edit.obo-iconv ....","title":"Build recipe of key goals"},{"location":"uberon_build_pipeline/#recurring-stuff-ignore-this","text":"Download mirrors (some preprocessing involved) Examples: tmp/fixed-emapa.obo tmp/mirror-emapa.obo tmp/mirror-zfa.obo tmp/fixed-zfa.obo imports/ncbitaxon_import.owl tmp/composite-stages.obo tmp/merged-stages-xrefs.obo tmp/update-stages tmp/developmental-stage-ontologies/src/ssso-merged.obo tmp/developmental-stage-ontologies/src/mmusdv/mmusdv.obo Imports tmp/seed.owl tmp/cl-core.obo","title":"Recurring stuff: (ignore this)"},{"location":"odk-workflows/","text":"Default ODK Workflows Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation","title":"Overview"},{"location":"odk-workflows/#default-odk-workflows","text":"Daily Editors Workflow Release Workflow Manage your ODK Repository Setting up Docker for ODK Imports management Managing the documentation","title":"Default ODK Workflows"},{"location":"odk-workflows/ContinuousIntegration/","text":"Introduction to Continuous Integration Workflows with ODK Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/uberon-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place!","title":"Continuous Integration"},{"location":"odk-workflows/ContinuousIntegration/#introduction-to-continuous-integration-workflows-with-odk","text":"Historically, most repos have been using Travis CI for continuous integration testing and building, but due to runtime restrictions, we recently switched a lot of our repos to GitHub actions. You can set up your repo with CI by adding this to your configuration file (src/ontology/uberon-odk.yaml): ci: - github_actions When updateing your repo , you will notice a new file being added: .github/workflows/qc.yml . This file contains your CI logic, so if you need to change, or add anything, this is the place!","title":"Introduction to Continuous Integration Workflows with ODK"},{"location":"odk-workflows/EditorsWorkflow/","text":"Editors Workflow The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future Local editing workflow Workflow requirements: - git - github - docker - editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc 1. Create issue Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change. 2. Update main branch In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout master git pull 3. Create feature branch Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess 4. Perform edit Using your editor of choice, perform the intended edit. For example: Protege Open src/ontology/uberon-edit.owl in Protege Make the change Save the file TextEdit Open src/ontology/uberon-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/uberon-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully. 4. Check the Git diff This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff 5. Quality control Now its time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ). 5a. Local testing If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test 6. Pull request When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by Github when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change). 7/5b. Continuous Integration Testing If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red. 8. Community review Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out! 9. Merge and cleanup When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request. 10. Changelog (Optional) It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc).","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#editors-workflow","text":"The editors workflow is one of the formal workflows to ensure that the ontology is developed correctly according to ontology engineering principles. There are a few different editors workflows: Local editing workflow: Editing the ontology in your local environment by hand, using tools such as Prot\u00e9g\u00e9, ROBOT templates or DOSDP patterns. Completely automated data pipeline (GitHub Actions) DROID workflow This document only covers the first editing workflow, but more will be added in the future","title":"Editors Workflow"},{"location":"odk-workflows/EditorsWorkflow/#local-editing-workflow","text":"Workflow requirements: - git - github - docker - editing tool of choice, e.g. Prot\u00e9g\u00e9, your favourite text editor, etc","title":"Local editing workflow"},{"location":"odk-workflows/EditorsWorkflow/#1-create-issue","text":"Ensure that there is a ticket on your issue tracker that describes the change you are about to make. While this seems optional, this is a very important part of the social contract of building an ontology - no change to the ontology should be performed without a good ticket, describing the motivation and nature of the intended change.","title":"1. Create issue"},{"location":"odk-workflows/EditorsWorkflow/#2-update-main-branch","text":"In your local environment (e.g. your laptop), make sure you are on the main (prev. master ) branch and ensure that you have all the upstream changes, for example: git checkout master git pull","title":"2. Update main branch"},{"location":"odk-workflows/EditorsWorkflow/#3-create-feature-branch","text":"Create a new branch. Per convention, we try to use meaningful branch names such as: - issue23removeprocess (where issue 23 is the related issue on GitHub) - issue26addcontributor - release20210101 (for releases) On your command line, this looks like this: git checkout -b issue23removeprocess","title":"3. Create feature branch"},{"location":"odk-workflows/EditorsWorkflow/#4-perform-edit","text":"Using your editor of choice, perform the intended edit. For example: Protege Open src/ontology/uberon-edit.owl in Protege Make the change Save the file TextEdit Open src/ontology/uberon-edit.owl in TextEdit (or Sublime, Atom, Vim, Nano) Make the change Save the file Consider the following when making the edit. According to our development philosophy, the only places that should be manually edited are: src/ontology/uberon-edit.owl Any ROBOT templates you chose to use (the TSV files only) Any DOSDP data tables you chose to use (the TSV files, and potentially the associated patterns) components (anything in src/ontology/components ), see here . Imports should not be edited (any edits will be flushed out with the next update). However, refreshing imports is a potentially breaking change - and is discussed elsewhere . Changes should usually be small. Adding or changing 1 term is great. Adding or changing 10 related terms is ok. Adding or changing 100 or more terms at once should be considered very carefully.","title":"4. Perform edit"},{"location":"odk-workflows/EditorsWorkflow/#4-check-the-git-diff","text":"This step is very important. Rather than simply trusting your change had the intended effect, we should always use a git diff as a first pass for sanity checking. In our experience, having a visual git client like GitHub Desktop or sourcetree is really helpful for this part. In case you prefer the command line: git status git diff","title":"4. Check the Git diff"},{"location":"odk-workflows/EditorsWorkflow/#5-quality-control","text":"Now its time to run your quality control checks. This can either happen locally ( 5a ) or through your continuous integration system ( 7/5b ).","title":"5. Quality control"},{"location":"odk-workflows/EditorsWorkflow/#5a-local-testing","text":"If you chose to run your test locally: sh run.sh make IMP=false test This will run the whole set of configured ODK tests on including your change. If you have a complex DOSDP pattern pipeline you may want to add PAT=false to skip the potentially lengthy process of rebuilding the patterns. sh run.sh make IMP=false PAT=false test","title":"5a. Local testing"},{"location":"odk-workflows/EditorsWorkflow/#6-pull-request","text":"When you are happy with the changes, you commit your changes to your feature branch, push them upstream (to GitHub) and create a pull request. For example: git add NAMEOFCHANGEDFILES git commit -m \"Added biological process term #12\" git push -u origin issue23removeprocess Then you go to your project on GitHub, and create a new pull request from the branch, for example: https://github.com/INCATools/ontology-development-kit/pulls There is a lot of great advise on how to write pull requests, but at the very least you should: - mention the tickets affected: see #23 to link to a related ticket, or fixes #23 if, by merging this pull request, the ticket is fixed. Tickets in the latter case will be closed automatically by Github when the pull request is merged. - summarise the changes in a few sentences. Consider the reviewer: what would they want to know right away. - If the diff is large, provide instructions on how to review the pull request best (sometimes, there are many changed files, but only one important change).","title":"6. Pull request"},{"location":"odk-workflows/EditorsWorkflow/#75b-continuous-integration-testing","text":"If you didn't run and local quality control checks (see 5a ), you should have Continuous Integration (CI) set up, for example: - Travis - GitHub Actions More on how to set this up here . Once the pull request is created, the CI will automatically trigger. If all is fine, it will show up green, otherwise red.","title":"7/5b. Continuous Integration Testing"},{"location":"odk-workflows/EditorsWorkflow/#8-community-review","text":"Once all the automatic tests have passed, it is important to put a second set of eyes on the pull request. Ontologies are inherently social - as in that they represent some kind of community consensus on how a domain is organised conceptually. This seems high brow talk, but it is very important that as an ontology editor, you have your work validated by the community you are trying to serve (e.g. your colleagues, other contributors etc). In our experience, it is hard to get more than one review on a pull request - two is great. You can set up GitHub branch protection to actually require a review before a pull request can be merged! We recommend this. This step seems daunting to some hopefully under-resourced ontologies, but we recommend to put this high up on your list of priorities - train a colleague, reach out!","title":"8. Community review"},{"location":"odk-workflows/EditorsWorkflow/#9-merge-and-cleanup","text":"When the QC is green and the reviews are in (approvals), it is time to merge the pull request. After the pull request is merged, remember to delete the branch as well (this option will show up as a big button right after you have merged the pull request). If you have not done so, close all the associated tickets fixed by the pull request.","title":"9. Merge and cleanup"},{"location":"odk-workflows/EditorsWorkflow/#10-changelog-optional","text":"It is sometimes difficult to keep track of changes made to an ontology. Some ontology teams opt to document changes in a changelog (simply a text file in your repository) so that when release day comes, you know everything you have changed. This is advisable at least for major changes (such as a new release system, a new pattern or template etc).","title":"10. Changelog (Optional)"},{"location":"odk-workflows/ManageDocumentation/","text":"Updating the Documentation The documentation for UBERON is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file cotains the documentation config, in particular its navigation bar and theme. The documentation is hosted using github pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch. Editing the docs Changing content All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into master. Deploy the documentation (see below) Deploy the documentation The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd uberon/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://obophenotype.github.io/uberon/ 3. Just to double check, you can now navigate to your documentation pages (usually https://obophenotype.github.io/uberon/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Managing the documentation"},{"location":"odk-workflows/ManageDocumentation/#updating-the-documentation","text":"The documentation for UBERON is managed in two places (relative to the repository root): The docs directory contains all the files that pertain to the content of the documentation (more below) the mkdocs.yaml file cotains the documentation config, in particular its navigation bar and theme. The documentation is hosted using github pages, on a special branch of the repository (called gh-pages ). It is important that this branch is never deleted - it contains all the files GitHub pages needs to render and deploy the site. It is also important to note that the gh-pages branch should never be edited manually . All changes to the docs happen inside the docs directory on the main branch.","title":"Updating the Documentation"},{"location":"odk-workflows/ManageDocumentation/#editing-the-docs","text":"","title":"Editing the docs"},{"location":"odk-workflows/ManageDocumentation/#changing-content","text":"All the documentation is contained in the docs directory, and is managed in Markdown . Markdown is a very simple and convenient way to produce text documents with formatting instructions, and is very easy to learn - it is also used, for example, in GitHub issues. This is a normal editing workflow: Open the .md file you want to change in an editor of choice (a simple text editor is often best). IMPORTANT : Do not edit any files in the docs/odk-workflows/ directory. These files are managed by the ODK system and will be overwritten when the repository is upgraded! If you wish to change these files, make an issue on the ODK issue tracker . Perform the edit and save the file Commit the file to a branch, and create a pull request as usual. If your development team likes your changes, merge the docs into master. Deploy the documentation (see below)","title":"Changing content"},{"location":"odk-workflows/ManageDocumentation/#deploy-the-documentation","text":"The documentation is not automatically updated from the Markdown, and needs to be deployed deliberately. To do this, perform the following steps: In your terminal, navigate to the edit directory of your ontology, e.g.: cd uberon/src/ontology Now you are ready to build the docs as follows: sh run.sh make update_docs Mkdocs now sets off to build the site from the markdown pages. You will be asked to Enter your username Enter your password (see here for using GitHub access tokens instead) IMPORTANT : Using password based authentication will be deprecated this year (2021). Make sure you read up on personal access tokens if that happens! If everything was successful, you will see a message similar to this one: INFO - Your documentation should shortly be available at: https://obophenotype.github.io/uberon/ 3. Just to double check, you can now navigate to your documentation pages (usually https://obophenotype.github.io/uberon/). Just make sure you give GitHub 2-5 minutes to build the pages!","title":"Deploy the documentation"},{"location":"odk-workflows/ReleaseWorkflow/","text":"The release workflow The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following. Run a release with the ODK Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to master are committed to Github ( git status should say that there are no modified files) Locally make sure you have the latest changes from master ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd uberon/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo). Review the release (Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (uberon.owl) in you favourite development environment (i.e. Protege) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): uberon.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! uberon-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at uberon-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR! Merge the main branch Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished). Create a GitHub release Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/obophenotype/uberon/releases. Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the uberon.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done. Debugging typical ontology release problems Problems with memory When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here . Problems when using OBO format based tools Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open uberon-edit.owl in Protege and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"Release Workflow"},{"location":"odk-workflows/ReleaseWorkflow/#the-release-workflow","text":"The release workflow recommended by the ODK is based on GitHub releases and works as follows: Run a release with the ODK Review the release Merge to main branch Create a GitHub release These steps are outlined in detail in the following.","title":"The release workflow"},{"location":"odk-workflows/ReleaseWorkflow/#run-a-release-with-the-odk","text":"Preparation: Ensure that all your pull requests are merged into your main (master) branch Make sure that all changes to master are committed to Github ( git status should say that there are no modified files) Locally make sure you have the latest changes from master ( git pull ) Checkout a new branch (e.g. git checkout -b release-2021-01-01 ) You may or may not want to refresh your imports as part of your release strategy (see here ) Make sure you have the latest ODK installed by running docker pull obolibrary/odkfull To actually run the release, you: Open a command line terminal window and navigate to the src/ontology directory ( cd uberon/src/ontology ) Run release pipeline: sh run.sh make prepare_release -B . Note that for some ontologies, this process can take up to 90 minutes - especially if there are large ontologies you depend on, like PRO or CHEBI. If everything went well, you should see the following output on your machine: Release files are now in ../.. - now you should commit, push and make a release on your git hosting site such as GitHub or GitLab . This will create all the specified release targets (OBO, OWL, JSON, and the variants, ont-full and ont-base) and copy them into your release directory (the top level of your repo).","title":"Run a release with the ODK"},{"location":"odk-workflows/ReleaseWorkflow/#review-the-release","text":"(Optional) Rough check. This step is frequently skipped, but for the more paranoid among us (like the author of this doc), this is a 3 minute additional effort for some peace of mind. Open the main release (uberon.owl) in you favourite development environment (i.e. Protege) and eyeball the hierarchy. We recommend two simple checks: Does the very top level of the hierarchy look ok? This means that all new terms have been imported/updated correctly. Does at least one change that you know should be in this release appear? For example, a new class. This means that the release was actually based on the recent edit file. Commit your changes to the branch and make a pull request In your GitHub pull request, review the following three files in detail (based on our experience): uberon.obo - this reflects a useful subset of the whole ontology (everything that can be covered by OBO format). OBO format has that speaking for it: it is very easy to review! uberon-base.owl - this reflects the asserted axioms in your ontology that you have actually edited. Ideally also take a look at uberon-full.owl , which may reveal interesting new inferences you did not know about. Note that the diff of this file is sometimes quite large. Like with every pull request, we recommend to always employ a second set of eyes when reviewing a PR!","title":"Review the release"},{"location":"odk-workflows/ReleaseWorkflow/#merge-the-main-branch","text":"Once your CI checks have passed, and your reviews are completed, you can now merge the branch into your main branch (don't forget to delete the branch afterwards - a big button will appear after the merge is finished).","title":"Merge the main branch"},{"location":"odk-workflows/ReleaseWorkflow/#create-a-github-release","text":"Go to your releases page on GitHub by navigating to your repository, and then clicking on releases (usually on the right, for example: https://github.com/obophenotype/uberon/releases. Then click \"Draft new release\" As the tag version you need to choose the date on which your ontologies were build. You can find this, for example, by looking at the uberon.obo file and check the data-version: property. The date needs to be prefixed with a v , so, for example v2020-02-06 . You can write whatever you want in the release title, but we typically write the date again. The description underneath should contain a concise list of changes or term additions. Click \"Publish release\". Done.","title":"Create a GitHub release"},{"location":"odk-workflows/ReleaseWorkflow/#debugging-typical-ontology-release-problems","text":"","title":"Debugging typical ontology release problems"},{"location":"odk-workflows/ReleaseWorkflow/#problems-with-memory","text":"When you are dealing with large ontologies, you need a lot of memory. When you see error messages relating to large ontologies such as CHEBI, PRO, NCBITAXON, or Uberon, you should think of memory first, see here .","title":"Problems with memory"},{"location":"odk-workflows/ReleaseWorkflow/#problems-when-using-obo-format-based-tools","text":"Sometimes you will get cryptic error messages when using legacy tools using OBO format, such as the ontology release tool (OORT), which is also available as part of the ODK docker container. In these cases, you need to track down what axiom or annotation actually caused the breakdown. In our experience (in about 60% of the cases) the problem lies with duplicate annotations ( def , comment ) which are illegal in OBO. Here is an example recipe of how to deal with such a problem: If you get a message like make: *** [cl.Makefile:84: oort] Error 255 you might have a OORT error. To debug this, in your terminal enter sh run.sh make IMP=false PAT=false oort -B (assuming you are already in the ontology folder in your directory) This should show you where the error is in the log (eg multiple different definitions) WARNING: THE FIX BELOW IS NOT IDEAL, YOU SHOULD ALWAYS TRY TO FIX UPSTREAM IF POSSIBLE Open uberon-edit.owl in Protege and find the offending term and delete all offending issue (e.g. delete ALL definition, if the problem was \"multiple def tags not allowed\") and save. *While this is not idea, as it will remove all definitions from that term, it will be added back again when the term is fixed in the ontology it was imported from and added back in. Rerun sh run.sh make IMP=false PAT=false oort -B and if it all passes, commit your changes to a branch and make a pull request as usual.","title":"Problems when using OBO format based tools"},{"location":"odk-workflows/RepoManagement/","text":"Managing your ODK repository Updating your ODK repository Your ODK repositories configuration is managed in src/ontology/uberon-odk.yaml . Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here. Managing imports You can use the update repository worflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following. Add new import To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an imports statement to your src/ontology/uberon-edit.owl file. We suggest to do this using a text editor, by simply copying an existing imports declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<http://purl.obolibrary.org/obo/uberon.owl> Import(<http://purl.obolibrary.org/obo/uberon/imports/ro_import.owl>) Import(<http://purl.obolibrary.org/obo/uberon/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/uberon/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported. Modify an existing import If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\". Remove an existing import To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/uberon-edit.owl . remove the id from your src/ontology/uberon-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file. Customise an import By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/uberon-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by the your ontology (see here ] on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/uberon-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/uberon.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline. Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Manage your ODK Repository"},{"location":"odk-workflows/RepoManagement/#managing-your-odk-repository","text":"","title":"Managing your ODK repository"},{"location":"odk-workflows/RepoManagement/#updating-your-odk-repository","text":"Your ODK repositories configuration is managed in src/ontology/uberon-odk.yaml . Once you have made your changes, you can run the following to apply your changes to the repository: sh run.sh make update_repo There are a large number of options that can be set to configure your ODK, but we will only discuss a few of them here.","title":"Updating your ODK repository"},{"location":"odk-workflows/RepoManagement/#managing-imports","text":"You can use the update repository worflow described on this page to perform the following operations to your imports: Add a new import Modify an existing import Remove an import you no longer want Customise an import We will discuss all these workflows in the following.","title":"Managing imports"},{"location":"odk-workflows/RepoManagement/#add-new-import","text":"To add a new import, you first edit your odk config as described above , adding an id to the product list in the import_group section (for the sake of this example, we assume you already import RO, and your goal is to also import GO): import_group: products: - id: ro - id: go Note: our ODK file should only have one import_group which can contain multiple imports (in the products section). Next, you run the update repo workflow to apply these changes. Note that by default, this module is going to be a SLME Bottom module, see here . To change that or customise your module, see section \"Customise an import\". To finalise the addition of your import, perform the following steps: Add an imports statement to your src/ontology/uberon-edit.owl file. We suggest to do this using a text editor, by simply copying an existing imports declaration and renaming it to the new ontology import, for example as follows: ... Ontology(<http://purl.obolibrary.org/obo/uberon.owl> Import(<http://purl.obolibrary.org/obo/uberon/imports/ro_import.owl>) Import(<http://purl.obolibrary.org/obo/uberon/imports/go_import.owl>) ... Add your imports redirect to your catalog file src/ontology/catalog-v001.xml , for example: <uri name=\"http://purl.obolibrary.org/obo/uberon/imports/go_import.owl\" uri=\"imports/go_import.owl\"/> Test whether everything is in order: Refresh your import Open in your Ontology Editor of choice (Protege) and ensure that the expected terms are imported.","title":"Add new import"},{"location":"odk-workflows/RepoManagement/#modify-an-existing-import","text":"If you simply wish to refresh your import in light of new terms, see here . If you wish to change the type of your module see section \"Customise an import\".","title":"Modify an existing import"},{"location":"odk-workflows/RepoManagement/#remove-an-existing-import","text":"To remove an existing import, perform the following steps: remove the import declaration from your src/ontology/uberon-edit.owl . remove the id from your src/ontology/uberon-odk.yaml , eg. - id: go from the list of products in the import_group . run update repo workflow delete the associated files manually: src/imports/go_import.owl src/imports/go_terms.txt Remove the respective entry from the src/ontology/catalog-v001.xml file.","title":"Remove an existing import"},{"location":"odk-workflows/RepoManagement/#customise-an-import","text":"By default, an import module extracted from a source ontology will be a SLME module, see here . There are various options to change the default. The following change to your repo config ( src/ontology/uberon-odk.yaml ) will switch the go import from an SLME module to a simple ROBOT filter module: import_group: products: - id: ro - id: go module_type: filter A ROBOT filter module is, essentially, importing all external terms declared by the your ontology (see here ] on how to declare external terms to be imported). Note that the filter module does not consider terms/annotations from namespaces other than the base-namespace of the ontology itself. For example, in the example of GO above, only annotations / axioms related to the GO base IRI (http://purl.obolibrary.org/obo/GO_) would be considered. This behaviour can be changed by adding additional base IRIs as follows: import_group: products: - id: go module_type: filter base_iris: - http://purl.obolibrary.org/obo/GO_ - http://purl.obolibrary.org/obo/CL_ - http://purl.obolibrary.org/obo/BFO If you wish to customise your import entirely, you can specify your own ROBOT command to do so. To do that, add the following to your repo config ( src/ontology/uberon-odk.yaml ): import_group: products: - id: ro - id: go module_type: custom Now add a new goal in your custom Makefile ( src/ontology/uberon.Makefile , not src/ontology/Makefile ). imports/go_import.owl: mirror/ro.owl imports/ro_terms_combined.txt if [ $(IMP) = true ]; then $(ROBOT) query -i $< --update ../sparql/preprocess-module.ru \\ extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ query --update ../sparql/inject-subset-declaration.ru --update ../sparql/postprocess-module.ru \\ annotate --ontology-iri $(ONTBASE)/$@ $(ANNOTATE_ONTOLOGY_VERSION) --output $@.tmp.owl && mv $@.tmp.owl $@; fi Now feel free to change this goal to do whatever you wish it to do! It probably makes some sense (albeit not being a strict necessity), to leave most of the goal instead and replace only: extract -T imports/ro_terms_combined.txt --force true --individuals exclude --method BOT \\ to another ROBOT pipeline. Note : if your mirror is particularly large and complex, read this ODK recommendation .","title":"Customise an import"},{"location":"odk-workflows/RepositoryFileStructure/","text":"Repository structure The main kinds of files in the repository: Release files Imports Components Release files Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed descripts of the release artefacts can be found here . Imports Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in UBERON Import URL Type pr http://purl.obolibrary.org/obo/pr.owl None cl http://purl.obolibrary.org/obo/cl.owl None go http://purl.obolibrary.org/obo/go.owl None envo http://purl.obolibrary.org/obo/envo.owl None ro http://purl.obolibrary.org/obo/ro.owl None bspo http://purl.obolibrary.org/obo/bspo.owl None chebi http://purl.obolibrary.org/obo/chebi.owl None pato http://purl.obolibrary.org/obo/pato.owl None ncbitaxon http://purl.obolibrary.org/obo/ncbitaxon.owl None nbo http://purl.obolibrary.org/obo/nbo.owl None ceph http://purl.obolibrary.org/obo/ceph.owl None cteno http://purl.obolibrary.org/obo/cteno.owl None ehdaa2 http://purl.obolibrary.org/obo/ehdaa2.owl None emapa http://purl.obolibrary.org/obo/emapa.owl None fbbt http://purl.obolibrary.org/obo/fbbt.owl None fbdv http://purl.obolibrary.org/obo/fbdv.owl None ma http://purl.obolibrary.org/obo/ma.owl None poro http://purl.obolibrary.org/obo/poro.owl None wbbt http://purl.obolibrary.org/obo/wbbt.owl None wbls http://purl.obolibrary.org/obo/wbls.owl None xao http://purl.obolibrary.org/obo/xao.owl None zfa http://purl.obolibrary.org/obo/zfa.owl None caro http://purl.obolibrary.org/obo/caro.owl None Components Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may chose to manage logic that is beyond OBO in a specific OWL component.","title":"Your ODK Repository Overview"},{"location":"odk-workflows/RepositoryFileStructure/#repository-structure","text":"The main kinds of files in the repository: Release files Imports Components","title":"Repository structure"},{"location":"odk-workflows/RepositoryFileStructure/#release-files","text":"Release file are the file that are considered part of the official ontology release and to be used by the community. A detailed descripts of the release artefacts can be found here .","title":"Release files"},{"location":"odk-workflows/RepositoryFileStructure/#imports","text":"Imports are subsets of external ontologies that contain terms and axioms you would like to re-use in your ontology. These are considered \"external\", like dependencies in software development, and are not included in your \"base\" product, which is the release artefact which contains only those axioms that you personally maintain. These are the current imports in UBERON Import URL Type pr http://purl.obolibrary.org/obo/pr.owl None cl http://purl.obolibrary.org/obo/cl.owl None go http://purl.obolibrary.org/obo/go.owl None envo http://purl.obolibrary.org/obo/envo.owl None ro http://purl.obolibrary.org/obo/ro.owl None bspo http://purl.obolibrary.org/obo/bspo.owl None chebi http://purl.obolibrary.org/obo/chebi.owl None pato http://purl.obolibrary.org/obo/pato.owl None ncbitaxon http://purl.obolibrary.org/obo/ncbitaxon.owl None nbo http://purl.obolibrary.org/obo/nbo.owl None ceph http://purl.obolibrary.org/obo/ceph.owl None cteno http://purl.obolibrary.org/obo/cteno.owl None ehdaa2 http://purl.obolibrary.org/obo/ehdaa2.owl None emapa http://purl.obolibrary.org/obo/emapa.owl None fbbt http://purl.obolibrary.org/obo/fbbt.owl None fbdv http://purl.obolibrary.org/obo/fbdv.owl None ma http://purl.obolibrary.org/obo/ma.owl None poro http://purl.obolibrary.org/obo/poro.owl None wbbt http://purl.obolibrary.org/obo/wbbt.owl None wbls http://purl.obolibrary.org/obo/wbls.owl None xao http://purl.obolibrary.org/obo/xao.owl None zfa http://purl.obolibrary.org/obo/zfa.owl None caro http://purl.obolibrary.org/obo/caro.owl None","title":"Imports"},{"location":"odk-workflows/RepositoryFileStructure/#components","text":"Components, in contrast to imports, are considered full members of the ontology. This means that any axiom in a component is also included in the ontology base - which means it is considered native to the ontology. While this sounds complicated, consider this: conceptually, no component should be part of more than one ontology. If that seems to be the case, we are most likely talking about an import. Components are often not needed for ontologies, but there are some use cases: There is an automated process that generates and re-generates a part of the ontology A part of the ontology is managed in ROBOT templates The expressivity of the component is higher than the format of the edit file. For example, people still choose to manage their ontology in OBO format (they should not) missing out on a lot of owl features. They may chose to manage logic that is beyond OBO in a specific OWL component.","title":"Components"},{"location":"odk-workflows/SettingUpDockerForODK/","text":"Setting up your Docker environment for ODK use One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/uberon-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up Docker for ODK"},{"location":"odk-workflows/SettingUpDockerForODK/#setting-up-your-docker-environment-for-odk-use","text":"One of the most frequent problems with running the ODK for the first time is failure because of lack of memory. This can look like a Java OutOfMemory exception, but more often than not it will appear as something like an Error 137 . There are two places you need to consider to set your memory: Your src/ontology/run.sh (or run.bat) file. You can set the memory in there by adding robot_java_args: '-Xmx8G' to your src/ontology/uberon-odk.yaml file, see for example here . Set your docker memory. By default, it should be about 10-20% more than your robot_java_args variable. You can manage your memory settings by right-clicking on the docker whale in your system bar-->Preferences-->Resources-->Advanced, see picture below.","title":"Setting up your Docker environment for ODK use"},{"location":"odk-workflows/UpdateImports/","text":"Update Imports Workflow This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here . Importing a new term Importing a new term is split into to sub-phases: Declaring the terms to be imported Refreshing imports dynamically Declaring terms to be imported There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Protege-based declaration Using term files Using the custom import template Protege-based declaration This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . Open your ontology (edit file) in Protege (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc) for this term are imported from the respective external source ontology and becomes visible in your ontology. Using term files Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported. Using the custom import template This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/uberon-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say current import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extent the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file . Refresh imports If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B","title":"Imports management"},{"location":"odk-workflows/UpdateImports/#update-imports-workflow","text":"This page discusses how to update the contents of your imports, like adding or removing terms. If you are looking to customise imports, like changing the module type, see here .","title":"Update Imports Workflow"},{"location":"odk-workflows/UpdateImports/#importing-a-new-term","text":"Importing a new term is split into to sub-phases: Declaring the terms to be imported Refreshing imports dynamically","title":"Importing a new term"},{"location":"odk-workflows/UpdateImports/#declaring-terms-to-be-imported","text":"There are three ways to declare terms that are to be imported from an external ontology. Choose the appropriate one for your particular scenario (all three can be used in parallel if need be): Protege-based declaration Using term files Using the custom import template","title":"Declaring terms to be imported"},{"location":"odk-workflows/UpdateImports/#protege-based-declaration","text":"This workflow is to be avoided, but may be appropriate if the editor does not have access to the ODK docker container . Open your ontology (edit file) in Protege (5.5+). Select 'owl:Thing' Add a new class as usual. Paste the full iri in the 'Name:' field, for example, http://purl.obolibrary.org/obo/CHEBI_50906. Click 'OK' Now you can use this term for example to construct logical definitions. The next time the imports are refreshed (see how to refresh here ), the metadata (labels, definitions, etc) for this term are imported from the respective external source ontology and becomes visible in your ontology.","title":"Protege-based declaration"},{"location":"odk-workflows/UpdateImports/#using-term-files","text":"Every import has, by default a term file associated with it, which can be found in the imports directory. For example, if you have a GO import in src/ontology/go_import.owl , you will also have an associated term file src/ontology/go_terms.txt . You can add terms in there simply as a list: GO:0008150 GO:0008151 Now you can run the refresh imports workflow ) and the two terms will be imported.","title":"Using term files"},{"location":"odk-workflows/UpdateImports/#using-the-custom-import-template","text":"This workflow is appropriate if: You prefer to manage all your imported terms in a single file (rather than multiple files like in the \"Using term files\" workflow above). You wish to augment your imported ontologies with additional information. This requires a cautionary discussion. To enable this workflow, you add the following to your ODK config file ( src/ontology/uberon-odk.yaml ), and update the repository : use_custom_import_module: TRUE Now you can manage your imported terms directly in the custom external terms template, which is located at src/templates/external_import.owl . Note that this file is a ROBOT template , and can, in principle, be extended to include any axioms you like. Before extending the template, however, read the following carefully. The main purpose of the custom import template is to enable the management off all terms to be imported in a centralised place. To enable that, you do not have to do anything other than maintaining the template. So if you, say current import APOLLO_SV:00000480 , and you wish to import APOLLO_SV:00000532 , you simply add a row like this: ID Entity Type ID TYPE APOLLO_SV:00000480 owl:Class APOLLO_SV:00000532 owl:Class When the imports are refreshed see imports refresh workflow , the term(s) will simply be imported from the configured ontologies. Now, if you wish to extent the Makefile (which is beyond these instructions) and add, say, synonyms to the imported terms, you can do that, but you need to (a) preserve the ID and ENTITY columns and (b) ensure that the ROBOT template is valid otherwise, see here . WARNING . Note that doing this is a widespread antipattern (see related issue ). You should not change the axioms of terms that do not belong into your ontology unless necessary - such changes should always be pushed into the ontology where they belong. However, since people are doing it, whether the OBO Foundry likes it or not, at least using the custom imports module as described here localises the changes to a single simple template and ensures that none of the annotations added this way are merged into the base file .","title":"Using the custom import template"},{"location":"odk-workflows/UpdateImports/#refresh-imports","text":"If you want to refresh the import yourself (this may be necessary to pass the travis tests), and you have the ODK installed, you can do the following (using go as an example): First, you navigate in your terminal to the ontology directory (underneath src in your hpo root directory). cd src/ontology Then, you regenerate the import that will now include any new terms you have added. sh run.sh make PAT=false imports/go_import.owl -B Since ODK 1.2.27, it is also possible to simply run the following, which is the same as the above: sh run.sh make refresh-go Note that in case you changed the defaults, you need to add IMP=true and/or MIR=true to the command below: sh run.sh make IMP=true MIR=true PAT=false imports/go_import.owl -B If you wish to skip refreshing the mirror, i.e. skip downloading the latest version of the source ontology for your import (e.g. go.owl for your go import) you can set MIR=false instead, which will do the exact same thing as the above, but is easier to remember: sh run.sh make IMP=true MIR=false PAT=false imports/go_import.owl -B","title":"Refresh imports"}]}